{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"TheDaVinciCode.txt\", \"r\", encoding=\"utf-8\")\n",
    "\n",
    "book_text = f.read()\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load a pre-trained BERT model that has been fine-tuned for NER\n",
    "model = BertForTokenClassification.from_pretrained(\"bert-base-cased\", num_labels=18)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_text = book_text.replace(\"\\n\", \" \")\n",
    "book_text = book_text.replace(\"  \", \" \")\n",
    "book_text = book_text[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/kristianvankuijk/.cache/torch/hub/huggingface_pytorch-transformers_main\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 43\u001b[0m\n\u001b[1;32m     38\u001b[0m     outputs\u001b[39m.\u001b[39mappend(output[\u001b[39m0\u001b[39m])\n\u001b[1;32m     40\u001b[0m \u001b[39m# concatenate outputs and extract relevant information\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[39m# output_tensor = torch.cat(outputs, dim=1)\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[39m# output_tokens = tokenizer.convert_ids_to_tokens(output_tensor[0])\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m output_tensor \u001b[39m=\u001b[39m model(input_ids, attention_mask\u001b[39m=\u001b[39;49mattention_mask)\n\u001b[1;32m     44\u001b[0m output_tokens \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mconvert_ids_to_tokens(output_tensor[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mtolist())\n\u001b[1;32m     46\u001b[0m \u001b[39m# find all instances of \"Robert Langdon\"\u001b[39;00m\n",
      "File \u001b[0;32m/Volumes/Drive/GitHub/DaVinciCodeTheTrackOfRobertLangdon/env/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Volumes/Drive/GitHub/DaVinciCodeTheTrackOfRobertLangdon/env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:968\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    966\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mYou cannot specify both input_ids and inputs_embeds at the same time\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    967\u001b[0m \u001b[39melif\u001b[39;00m input_ids \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 968\u001b[0m     input_shape \u001b[39m=\u001b[39m input_ids\u001b[39m.\u001b[39;49msize()\n\u001b[1;32m    969\u001b[0m \u001b[39melif\u001b[39;00m inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    970\u001b[0m     input_shape \u001b[39m=\u001b[39m inputs_embeds\u001b[39m.\u001b[39msize()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "# set maximum sequence length and BERT model\n",
    "max_seq_length = 512\n",
    "model = torch.hub.load('huggingface/pytorch-transformers', 'model', 'bert-base-cased')\n",
    "\n",
    "# tokenize book text and split into smaller sequences\n",
    "\n",
    "# Tokenize the input text using the BERT tokenizer\n",
    "tokens = tokenizer.tokenize(book_text)\n",
    "\n",
    "# Add the special [CLS] and [SEP] tokens to mark the beginning and end of the input text\n",
    "tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "segments = [0] * len(token_ids)\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "while len(token_ids) > 0:\n",
    "    if len(token_ids) > max_seq_length - 2:\n",
    "        input_id = [tokenizer.cls_token_id] + token_ids[:max_seq_length-2] + [tokenizer.sep_token_id]\n",
    "        attention_mask = [1] * len(input_id)\n",
    "        token_ids = token_ids[max_seq_length-2:]\n",
    "    else:\n",
    "        input_id = [tokenizer.cls_token_id] + token_ids + [tokenizer.sep_token_id]\n",
    "        attention_mask = [1] * len(input_id)\n",
    "        padding = [0] * (max_seq_length - len(input_id))\n",
    "        input_id += padding\n",
    "        attention_mask += padding\n",
    "        token_ids = []\n",
    "    input_ids.append(input_id)\n",
    "    attention_masks.append(attention_mask)\n",
    "\n",
    "# convert input sequences to tensors and feed into model\n",
    "outputs = []\n",
    "for i in range(len(input_ids)):\n",
    "    input_ids_tensor = torch.tensor([input_ids[i]])\n",
    "    attention_mask_tensor = torch.tensor([attention_masks[i]])\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids_tensor, attention_mask=attention_mask_tensor)\n",
    "    outputs.append(output[0])\n",
    "\n",
    "# concatenate outputs and extract relevant information\n",
    "# output_tensor = torch.cat(outputs, dim=1)\n",
    "# output_tokens = tokenizer.convert_ids_to_tokens(output_tensor[0])\n",
    "output_tensor = model(input_ids, attention_mask=attention_mask)\n",
    "output_tokens = tokenizer.convert_ids_to_tokens(output_tensor[0].tolist())\n",
    "\n",
    "# find all instances of \"Robert Langdon\"\n",
    "robert_langdon_indices = []\n",
    "print(output_tensor)\n",
    "for i, token in enumerate(output_tokens):\n",
    "    # if token == \"robert\" and output_tokens[i+1] == \"langdon\":\n",
    "    if token == \"robert\":\n",
    "        robert_langdon_indices.append(i)\n",
    "\n",
    "# print the sentence containing each instance of \"Robert Langdon\"\n",
    "for index in robert_langdon_indices:\n",
    "    start = max(0, index - 20)\n",
    "    end = min(len(output_tokens), index + 20)\n",
    "    sentence = model.tokenizer.convert_tokens_to_string(output_tokens[start:end])\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "robert_langdon_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
